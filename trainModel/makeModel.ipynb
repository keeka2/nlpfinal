{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "nlpy = Okt()\n",
    "file = open('hashcode_classification2020_train.csv', 'r', encoding='utf-8')\n",
    "line = csv.reader(file)\n",
    "\n",
    "#전처리하여 저장\n",
    "f = open(\"goodWordsk.txt\", 'w')\n",
    "\n",
    "english=open(\"English_Dict.txt\", 'r')\n",
    "korean=open(\"Korean_Dict.txt\", 'r')\n",
    "special=open(\"Special_Dict.txt\", 'r', encoding='utf-8')\n",
    "\n",
    "#영어사전, 한글사전, 특수문자 사전 불러와서 저장\n",
    "englishDict=[]\n",
    "sentences = english.readlines()\n",
    "for sentence in sentences: #한 문장씩\n",
    "    englishDict.append(sentence[:-1])\n",
    "\n",
    "koreanDict=[]\n",
    "sentences = korean.readlines()\n",
    "for sentence in sentences: #한 문장씩\n",
    "    koreanDict.append(sentence[:-1])\n",
    "\n",
    "specialDict=[]\n",
    "sentences = special.readlines()\n",
    "for sentence in sentences: #한 문장씩\n",
    "    specialDict.append(sentence[:-1])\n",
    "\n",
    "#각 게시물의 (한글/ 영어/ 특수문자) 중 (한글 사전/ 영어 사전/ 특수문자 사전)에 있는 단어들만 추출\n",
    "for line_text in line:\n",
    "    #영어만 추출\n",
    "    eng=re.sub('[^a-zA-Z]',' ',line_text[0]+line_text[1]).strip()\n",
    "    eng_temp=eng.split(' ')\n",
    "    eng_temp = list(filter(None, eng_temp))\n",
    "    for i in range(len(eng_temp)):\n",
    "        if eng_temp[i] not in englishDict:\n",
    "            eng_temp[i]=''\n",
    "    eng_temp = list(filter(None, eng_temp))\n",
    "    \n",
    "    #한글만 추출\n",
    "    kor=nlpy.nouns(line_text[0]+line_text[1])\n",
    "    for i in range(len(kor)):\n",
    "        if kor[i] not in koreanDict:\n",
    "            kor[i]=''\n",
    "    kor = list(filter(None, kor))\n",
    "    \n",
    "    #특수문자만 추출\n",
    "    spe = re.sub('[a-zA-Z0-9]',' ',line_text[0]+line_text[1])\n",
    "    spe = re.sub('[ㄱ-ㅣ가-힣]+',' ',spe)\n",
    "    special_temp = spe.split(' ')\n",
    "    special_temp = list(filter(None, special_temp))\n",
    "    for i in range(len(special_temp)):\n",
    "        if special_temp[i] not in specialDict:\n",
    "            special_temp[i]=''\n",
    "    special_temp = list(filter(None, special_temp))\n",
    "    \n",
    "    s=' '.join(eng_temp)+' '+' '.join(kor)+' '+' '.join(special_temp)+','+line_text[2]\n",
    "    f.write(s)\n",
    "    f.write('\\n')\n",
    "    \n",
    "    \n",
    "#     kor=\n",
    "file.close()\n",
    "f.close()\n",
    "english.close()\n",
    "korean.close()\n",
    "special.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'wait sleep wait sleep 차이점 및 차이점 ', 'flag': '3'}\n",
      "(2592,)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "#전처리한 문자 불러와서 학습시키기\n",
    "file= open('goodWordsk.txt', 'r')\n",
    "line = file.readline()\n",
    "training_data=[]\n",
    "while line:\n",
    "    data=[line[:-3],line[-2:-1]]\n",
    "    training_data.append({'data': data[0], 'flag': data[1]})\n",
    "    line=file.readline()\n",
    "print(training_data[0])\n",
    "\n",
    "training_data=pandas.DataFrame(training_data, columns=['data', 'flag'])\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "print(training_data.data.shape)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.data)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.01, random_state=17)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  -  5\n",
      "4  -  4\n",
      "5  -  5\n",
      "3  -  2\n",
      "3  -  3\n",
      "5  -  5\n",
      "3  -  1\n",
      "3  -  3\n",
      "5  -  3\n",
      "5  -  5\n",
      "1  -  1\n",
      "3  -  3\n",
      "1  -  2\n",
      "5  -  5\n",
      "5  -  5\n",
      "4  -  4\n",
      "5  -  5\n",
      "2  -  1\n",
      "2  -  2\n",
      "3  -  3\n",
      "5  -  5\n",
      "4  -  4\n",
      "3  -  3\n",
      "5  -  4\n",
      "3  -  3\n",
      "3  -  2\n"
     ]
    }
   ],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(predicted_item, ' - ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "docs_new=[]\n",
    "f=open(\"testwords.txt\",'r')\n",
    "sentences = f.readlines()\n",
    "for sentence in sentences:\n",
    "    docs_new.append([sentence[:-1]])\n",
    "f.close()\n",
    "f2 = open('output_l.csv', 'w', newline='')\n",
    "wr = csv.writer(f2)\n",
    "wr.writerow(['label'])\n",
    "for docs in docs_new:\n",
    "    X_new_counts = loaded_vec.transform(docs)\n",
    "    X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "    predicted = clf_neural.predict(X_new_tfidf)\n",
    "    wr.writerow([predicted[0]])\n",
    "\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
